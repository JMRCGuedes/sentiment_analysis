{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "The dataset has IMDB movie reviwes, already divided with 25K reviews for train and test and has the following structure:\n",
    "\n",
    "* train\n",
    "    * pos -> the positive classified reviews in the train set\n",
    "    * neg -> the negative classified reviews in the train set\n",
    "\n",
    "* test\n",
    "    * pos -> the positive classified reviews in the test set\n",
    "    * neg -> the negative classified reviews in the test set\n",
    "    \n",
    "So the first step is to import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import io as io\n",
    "import csv\n",
    "\n",
    "files = [file for file in glob.glob(\"./dataset/train/pos/*\")]\n",
    "index = 1\n",
    "\n",
    "for file_name in files:\n",
    "    # open file eoth review\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    # open csv file to store data\n",
    "    with open('train.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # pos -> 1; neg -> 0\n",
    "        if index == 1:\n",
    "            writer.writerow(['review', 'sentiment'])\n",
    "        writer.writerow([content, 1])\n",
    "    index = index + 1\n",
    "    \n",
    "files = [file for file in glob.glob(\"./dataset/train/neg/*\")]\n",
    "index = 1\n",
    "\n",
    "for file_name in files:\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    with open('train.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # pos -> 1; neg -> 0\n",
    "        writer.writerow([content, 0])\n",
    "    index = index + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import io as io\n",
    "import csv\n",
    "\n",
    "files = [file for file in glob.glob(\"./dataset/test/pos/*\")]\n",
    "index = 1\n",
    "\n",
    "for file_name in files:\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    with open('test.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # pos -> 1; neg -> 0\n",
    "        if index == 1:\n",
    "            writer.writerow(['review', 'sentiment'])\n",
    "        writer.writerow([content, 1])\n",
    "    index = index + 1\n",
    "    \n",
    "files = [file for file in glob.glob(\"./dataset/test/neg/*\")]\n",
    "index = 1\n",
    "\n",
    "for file_name in files:\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    with open('test.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # pos -> 1; neg -> 0\n",
    "        writer.writerow([content, 0])\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      b'Bromwell High is a cartoon comedy. It ran at...          1\n",
      "1      b'Homelessness (or Houselessness as George Car...          1\n",
      "2      b'Brilliant over-acting by Lesley Ann Warren. ...          1\n",
      "3      b'This is easily the most underrated film inn ...          1\n",
      "4      b'This is not the typical Mel Brooks film. It ...          1\n",
      "...                                                  ...        ...\n",
      "24995  b\"Towards the end of the movie, I felt it was ...          0\n",
      "24996  b'This is the kind of movie that my enemies co...          0\n",
      "24997  b\"I saw 'Descent' last night at the Stockholm ...          0\n",
      "24998  b\"Some films that you pick up for a pound turn...          0\n",
      "24999  b\"This is one of the dumbest films, I've ever ...          0\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                  review  sentiment\n",
      "0      b\"I went and saw this movie last night after b...          1\n",
      "1      b'Actor turned director Bill Paxton follows up...          1\n",
      "2      b'As a recreational golfer with some knowledge...          1\n",
      "3      b\"I saw this film in a sneak preview, and it i...          1\n",
      "4      b'Bill Paxton has taken the true story of the ...          1\n",
      "...                                                  ...        ...\n",
      "24995  b'I occasionally let my kids watch this garbag...          0\n",
      "24996  b\"When all we have anymore is pretty much real...          0\n",
      "24997  b\"The basic genre is a thriller intercut with ...          0\n",
      "24998  b'Four things intrigued me as to this film - f...          0\n",
      "24999  b'David Bryce\\'s comments nearby are exception...          0\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./dataset/train.csv\")\n",
    "test = pd.read_csv(\"./dataset/test.csv\")\n",
    "\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial text pre-processing \n",
    "\n",
    "* Remove 'b string and \\<br /> tag in each review\n",
    "* Lowercasing each review\n",
    "* Remove special characters (\", ', \\\\, (, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parseText(text):\n",
    "    '''\n",
    "    text = text.replace(\"b'\", \"\")\n",
    "    text = text.replace(\"b\\\"\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    text = text.replace(\"<br />\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\"  \", \"\")\n",
    "    text = text.lower()\n",
    "    '''\n",
    "    \n",
    "    text = text.replace(\"b'\", \"\")\n",
    "    text = text.replace(\"b\\\"\", \"\")\n",
    "    text = text.replace(\"<br />\", \"\")\n",
    "    text = text.replace(\"<br/>\", \"\")\n",
    "    text = text.replace(\"<br >\", \"\")\n",
    "    text = text.replace(\"<br>\", \"\")\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      bromwell high is a cartoon comedy it ran at th...          1\n",
      "1      homelessness or houselessness as george carlin...          1\n",
      "2      brilliant overacting by lesley ann warren best...          1\n",
      "3      this is easily the most underrated film inn th...          1\n",
      "4      this is not the typical mel brooks film it was...          1\n",
      "...                                                  ...        ...\n",
      "24995  towards the end of the movie i felt it was too...          0\n",
      "24996  this is the kind of movie that my enemies cont...          0\n",
      "24997  i saw descent last night at the stockholm film...          0\n",
      "24998  some films that you pick up for a pound turn o...          0\n",
      "24999  this is one of the dumbest films i ve ever see...          0\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(train['review']) ):\n",
    "    train.loc[i, 'review'] = parseText(train.loc[i, 'review'])\n",
    "    \n",
    "for i in range(0, len(test['review']) ):\n",
    "    test.loc[i, 'review'] = parseText(test.loc[i, 'review'])\n",
    "    \n",
    "deep_train = train\n",
    "deep_test = test\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create values to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train.loc[:24999, 'review'].values\n",
    "# y_train = train.loc[:24999, 'sentiment'].values.astype(int)    \n",
    "\n",
    "# X_test = test.loc[:24999, 'review'].values\n",
    "# y_test = test.loc[:24999, 'sentiment'].values.astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    x_data = dataset['review']   \n",
    "    y_data = dataset['sentiment']\n",
    "\n",
    "#     x_data = x_data.replace({'<.*?>': ''}, regex = True)\n",
    "#     x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])\n",
    "#     x_data = x_data.apply(lambda review: [w.lower() for w in review])\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "X_train, y_train = load_dataset(train)\n",
    "X_test, y_test = load_dataset(test)\n",
    "\n",
    "\n",
    "\n",
    "# X_test, y_test = load_dataset(twitter_data)\n",
    "# X_test, y_test = load_dataset(small_reviews)\n",
    "# X_test, y_test = load_dataset(small_train)\n",
    "\n",
    "\n",
    "# print('Reviews')\n",
    "# print(X_train, '\\n')\n",
    "# print('Sentiment')\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Padd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[30526   206   905 ...     0     0     0]\n",
      " [13250  2581   349 ...  5183   219   280]\n",
      " [  382  3574 16181 ...     0     0     0]\n",
      " ...\n",
      " [ 1307  3051    45 ...    14     1    17]\n",
      " [   29  1038  6393 ...     0     0     0]\n",
      " [    3  6731    29 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  283    97     1 ...     0     0     0]\n",
      " [   19 11465   414 ...   834    63   187]\n",
      " [  357  3411   794 ...   357    20   310]\n",
      " ...\n",
      " [10556  2454   690 ...    20    12    43]\n",
      " [  547    79  3765 ...     0     0     0]\n",
      " [ 5342  1487  7453 ...    32 78023   898]] \n",
      "\n",
      "Maximum review length:  118\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in X_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))\n",
    "\n",
    "\n",
    "\n",
    "token = Tokenizer(lower=False)\n",
    "token.fit_on_texts(X_train)\n",
    "token.fit_on_texts(X_test)\n",
    "X_train = token.texts_to_sequences(X_train)\n",
    "X_test = token.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', X_train, '\\n')\n",
    "print('Encoded X Test\\n', X_test, '\\n')\n",
    "print('Maximum review length: ', max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       [bromwell, high, is, a, cartoon, comedy, it, r...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n",
      "review       [i, went, and, saw, this, movie, last, night, ...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Instantiate a tweet tokenizer that will preserve each word (or token) as it is\n",
    "tweet_tokenizer = TweetTokenizer(\n",
    "    preserve_case = True,\n",
    "    reduce_len    = False,\n",
    "    strip_handles = False)\n",
    "\n",
    "# deep_train\n",
    "# deep_test\n",
    "\n",
    "classification_train = deep_train.copy()\n",
    "classification_test = deep_test.copy()\n",
    "\n",
    "classification_train['review'] = [tweet_tokenizer.tokenize(n) for n in classification_train['review']]\n",
    "classification_test['review'] = [tweet_tokenizer.tokenize(n) for n in classification_test['review']]\n",
    "\n",
    "# Example review\n",
    "print(classification_train.loc[0])\n",
    "print(classification_test.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       [(bromwell, RB), (high, JJ), (is, VBZ), (a, DT...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n",
      "review       [(i, JJ), (went, VBD), (and, CC), (saw, VBD), ...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n",
      "review       [bromwell, high, be, a, cartoon, comedy, it, r...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n",
      "review       [i, go, and, saw, this, movie, last, night, af...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag    # Part-of-speech tagger\n",
    "\n",
    "classification_train['review'] = [pos_tag(p) for p in classification_train['review']]\n",
    "classification_test['review'] = [pos_tag(p) for p in classification_test['review']]\n",
    "\n",
    "# Example review tagged\n",
    "print(classification_train.loc[0])\n",
    "print(classification_test.loc[0])\n",
    "\n",
    "\n",
    "# LEMMANIZATION\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# All we need is to know the type (Noun, Verb, or others) of each word\n",
    "def _tag2type(tag):\n",
    "    '''\n",
    "    Take a tag and return a type.\n",
    "    return 'n' for noun, 'v' for verb, and 'a' for any\n",
    "    '''\n",
    "    if tag.startswith('NN'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'a'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "classification_train['review'] = [[lemmatizer.lemmatize(word, _tag2type(tag)) for (word, tag) in tags] for tags in classification_train['review']]\n",
    "classification_test['review'] = [[lemmatizer.lemmatize(word, _tag2type(tag)) for (word, tag) in tags] for tags in classification_test['review']]\n",
    "\n",
    "# Example review\n",
    "print(classification_train.loc[0])\n",
    "print(classification_test.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       [bromwell, high, cartoon, comedy, run, time, p...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n",
      "review       [go, saw, movie, last, night, coax, friend, mi...\n",
      "sentiment                                                    1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NOISE REDUCTION\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# print\n",
    "stopwords[:10]\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def _is_noise(word):\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|(@[A-Za-z0-9_]+)'\n",
    "    return word in punctuation \\\n",
    "        or word.lower() in stopwords \\\n",
    "        or re.search(pattern, word, re.IGNORECASE) != None\n",
    "\n",
    "classification_train['review'] = [[p.lower() for p in _list if not _is_noise(p)] for _list in classification_train['review']]\n",
    "classification_test['review'] = [[p.lower() for p in _list if not _is_noise(p)] for _list in classification_test['review']]\n",
    "\n",
    "# Example review\n",
    "print(classification_train.loc[0])\n",
    "print(classification_test.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = classification_train['review']\n",
    "sentiments_train = classification_train['sentiment']\n",
    "\n",
    "reviews_test = classification_test['review']\n",
    "sentiments_test = classification_test['sentiment']\n",
    "\n",
    "save_train = classification_train.copy()\n",
    "save_test = classification_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing tran and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object get_tweets_for_model at 0x000001FDCB043E48>\n",
      "<generator object get_tweets_for_model at 0x000001FDD750D9C8>\n"
     ]
    }
   ],
   "source": [
    "def get_tweets_for_model(tokens_list):\n",
    "    '''\n",
    "    Generator function that associates a boolean 'True' to each token in a list of tokens,\n",
    "    which represents the label of each token.\n",
    "    This step is required by the NLTK classifier we'll be using:\n",
    "    - Documentation: https://www.nltk.org/book/ch06.html\n",
    "    \n",
    "    @arg tokens_list a 2-D list of (preferably cleaned) tokens\n",
    "    @return A 2-D list of tuples (original_token, True) containing the unaltered token and a boolean label\n",
    "    '''\n",
    "    for tweet_tokens in tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "tokens_for_model_train = get_tweets_for_model(reviews_train)\n",
    "tokens_for_model_test = get_tweets_for_model(reviews_test)\n",
    "\n",
    "print(tokens_for_model_train)\n",
    "print(tokens_for_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'bromwell': True, 'high': True, 'cartoon': True, 'comedy': True, 'run': True, 'time': True, 'program': True, 'school': True, 'life': True, 'teacher': True, '35': True, 'year': True, 'teaching': True, 'profession': True, 'lead': True, 'believe': True, 'satire': True, 'much': True, 'close': True, 'reality': True, 'scramble': True, 'survive': True, 'financially': True, 'insightful': True, 'student': True, 'see': True, 'right': True, 'pathetic': True, 'pomp': True, 'pettiness': True, 'whole': True, 'situation': True, 'remind': True, 'knew': True, 'saw': True, 'episode': True, 'repeatedly': True, 'try': True, 'burn': True, 'immediately': True, 'recall': True, 'classic': True, 'line': True, 'inspector': True, 'sack': True, 'one': True, 'welcome': True, 'expect': True, 'many': True, 'adult': True, 'age': True, 'think': True, 'far': True, 'fetch': True, 'pity': True}, 1)\n",
      "({'go': True, 'saw': True, 'movie': True, 'last': True, 'night': True, 'coax': True, 'friend': True, 'mine': True, 'admit': True, 'reluctant': True, 'see': True, 'knew': True, 'ashton': True, 'kutcher': True, 'able': True, 'comedy': True, 'wrong': True, 'play': True, 'character': True, 'jake': True, 'fischer': True, 'well': True, 'kevin': True, 'costner': True, 'played': True, 'ben': True, 'randall': True, 'professionalism': True, 'sign': True, 'good': True, 'toy': True, 'emotion': True, 'one': True, 'exactly': True, 'entire': True, 'theater': True, 'sell': True, 'overcome': True, 'laughter': True, 'first': True, 'half': True, 'move': True, 'tear': True, 'second': True, 'exit': True, 'many': True, 'woman': True, 'full': True, 'grow': True, 'men': True, 'try': True, 'desperately': True, 'let': True, 'anyone': True, 'cry': True, 'great': True, 'suggest': True, 'judge': True}, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert 1/0 to Positive/Negative\n",
    "def get_sentiment(sentiment):\n",
    "    if sentiment == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "# Append tokens_for_model to sentiment value\n",
    "count = 0\n",
    "prepared_model_train = []\n",
    "for value in tokens_for_model_train:\n",
    "    prepared_model_train.append((value, sentiments_train[count]))\n",
    "    count = count + 1\n",
    "    \n",
    "count = 0\n",
    "prepared_model_test = []\n",
    "for value in tokens_for_model_test:\n",
    "    prepared_model_test.append((value, sentiments_test[count]))\n",
    "    count = count + 1\n",
    "    \n",
    "print(prepared_model_train[0])\n",
    "print(prepared_model_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.94712\n",
      "\n",
      "Test Accuracy: 0.83708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = prepared_model_train\n",
    "test_data = prepared_model_test\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Training Accuracy\n",
    "print(\"Train Accuracy: {}\\n\".format(classify.accuracy(classifier, train_data)))\n",
    "# Testing Accuracy\n",
    "print(\"Test Accuracy: {}\\n\".format(classify.accuracy(classifier, test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tweet):\n",
    "    '''\n",
    "    Wrapper function for the pre-processing and classification steps previously performed.\n",
    "    \n",
    "    @arg tweet: String representing a tweet\n",
    "    @return String representing a polarity. (Positive or Negative)\n",
    "    '''\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word, _tag2type(tag)).lower()\n",
    "        for word, tag in pos_tag(tokens)\n",
    "        if not _is_noise(word)\n",
    "    ]\n",
    "    \n",
    "    return tokens, classifier.classify(dict([token, True] for token in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised tokens: ['movie', 'complete', 'waste', 'time']\n",
      "Polarity: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_tweet = \"this movie was a complete waste of time\"\n",
    "tokens, polarity = classify(example_tweet)\n",
    "\n",
    "print(\"Denoised tokens: {}\\nPolarity: {}\\n\".format(tokens, polarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning (LSTM with tensorfow and keras)\n",
    "\n",
    "* In this fhase, we are going to write a few model arquitectures to decide what is the best aproach\n",
    "\n",
    "* All the models have at least 3 layers:\n",
    "    * The first one is always an embedding layer to create the reviews embedding matrix to feed de network\n",
    "    * The middle layer is an Reccurent layer (LSTM/GRU) with different numbers of nodes for test porpuses (32, 64, etc,...)\n",
    "    * The last layer is a FC layer with one output node with sigmoidal activation\n",
    "    \n",
    "    \n",
    "* The loss function, the optimizer and the selected metric were the same in all aproaches to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproach 1\n",
    "\n",
    "* Add the Dropout layer in the middle of the main layers to prevent train data overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buil Model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 118, 32)           4263552   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 118, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              33000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 4,305,873\n",
      "Trainable params: 4,305,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# model.add(Bidirectional(LSTM(32, dropout=0.2)))\n",
    "\n",
    "print('Buil Model...')\n",
    "\n",
    "EMBED_DIM = 32\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(input_dim=total_words, output_dim=EMBED_DIM, input_length=max_length))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(GRU(32, dropout=0.2))\n",
    "model_1.add(Dense(units=1000, activation='relu'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_1.summary()\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproach 2\n",
    "\n",
    "* Simple aproach with GRU layer instead of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 118, 32)           6628864   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,653,761\n",
      "Trainable params: 6,653,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model_2.add(LSTM(LSTM_OUT, dropout=0.2))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproach 3\n",
    "\n",
    "* LSTM simple aproach with some changes in the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 118, 100)          13323600  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 13,564,601\n",
      "Trainable params: 13,564,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_DIM = 100\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model_3.add(LSTM(200, dropout=0.2))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproach 4\n",
    "\n",
    "* This aproach adds layers usually used in CNN to discover some relevant features that could make batter classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 118, 32)           4263552   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 118, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 59, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,319,957\n",
      "Trainable params: 4,319,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_DIM = 32\n",
    "\n",
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "model_4.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_4.add(MaxPooling1D(pool_size=2))\n",
    "model_4.add(LSTM(100))\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test one of the models\n",
    "\n",
    "* The second model (Aproach 2) is the model with the best accurary so we are going to use that one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5165 - accuracy: 0.7117\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 33s 169ms/step - loss: 0.2079 - accuracy: 0.9272\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 34s 171ms/step - loss: 0.0964 - accuracy: 0.9727\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.0620 - accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.0390 - accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fde9c928c8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 128, epochs = 5)\n",
    "\n",
    "# model.save('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "\n",
    "* We assume for test porposes that if the result of the output layer (between 0 and 1) is grater tha 0,5 than the review as a positive senitment, otherwise negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 20827\n",
      "Wrong Prediction: 4173\n",
      "Accuracy: 83.308\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict_classes(X_test, batch_size = 128)\n",
    "\n",
    "def test():\n",
    "    y_pred = (model.predict(X_test, batch_size = 128) > 0.5).astype(\"int32\")\n",
    "\n",
    "    true = 0\n",
    "    for i, y in enumerate(y_test):\n",
    "        if y == y_pred[i]:\n",
    "            true += 1\n",
    "\n",
    "    print('Correct Prediction: {}'.format(true))\n",
    "    print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "    print('Accuracy: {}'.format(true/len(y_pred)*100))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test\n",
    "\n",
    "* Run this so you can write an review an see what is the senitment of that review (IMDB review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life one of the worst movies i ever seen in my life\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4068194]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review=input()\n",
    "data = [[review, 1]]\n",
    "\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "type_test = pd.DataFrame(data, columns = ['review', 'sentiment'])\n",
    "\n",
    "X_type_test, y_type_test = load_dataset(type_test)\n",
    "\n",
    "token = Tokenizer(lower=False)\n",
    "token.fit_on_texts(X_type_test)\n",
    "X_type_test = token.texts_to_sequences(X_type_test)\n",
    "X_type_test = pad_sequences(X_type_test, maxlen=max_length, padding='post')\n",
    "\n",
    "y_pred = (model.predict(X_type_test, batch_size = 128) > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "print(model.predict(X_type_test, batch_size = 128))\n",
    "\n",
    "\n",
    "if y_pred[0][0] == 0:\n",
    "    print('Negative sentiment')\n",
    "else:\n",
    "    print('Positive sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter reviews\n",
    "\n",
    "* The next step is to use our model to classify twitter movie reviews but the model was trained in imdb review (usually bigger, diferent format, ect,...)\n",
    "\n",
    "1. Create test dataset with only the reviews that have 250 words or less to check the results \n",
    "\n",
    "2. To test the model if it gives good results with twiiter reviwes, we teste with nltk library\n",
    "    * The nltk ussualy used to preprocessing, has a model of twitter reviwes, so we creare a test set with 1000 positive and 1000 negative reviews to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test dataset with only the reviews that have 250 words or less\n",
    "\n",
    "* To test the model behavior with reviews that look like Twitter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_small_reviews_dataframe(frame):\n",
    "    data = {'review': [], 'sentiment': []}\n",
    "\n",
    "    aux = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    for i in range(0, len(frame['review']) ):\n",
    "        if len(frame.loc[i, 'review']) < 250:\n",
    "            new_row = {'review': frame.loc[i, 'review'], 'sentiment': int(frame.loc[i, 'sentiment'])}\n",
    "            aux = aux.append(new_row, ignore_index=True)\n",
    "\n",
    "    return aux\n",
    "\n",
    "small_train = small_train.iloc[0:0]\n",
    "small_test = small_test.iloc[0:0]\n",
    "small_train = get_small_reviews_dataframe(train)\n",
    "small_test = get_small_reviews_dataframe(test)\n",
    "\n",
    "print(small_train)\n",
    "print(small_test)\n",
    "\n",
    "test = test.iloc[0:0]\n",
    "test = pd.concat([small_train, small_test], ignore_index=True)\n",
    "# test = small_test\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Nltk twitter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review  sentiment\n",
      "0    films adapted from comic books have had plenty...        1.0\n",
      "1    every now and then a movie comes along from a ...        1.0\n",
      "2    you ' ve got mail works alot better than it de...        1.0\n",
      "3    \" jaws \" is a rare film that grabs your attent...        1.0\n",
      "4    moviemaking is a lot like being the general ma...        1.0\n",
      "..                                                 ...        ...\n",
      "995  wow ! what a movie . it ' s everything a movie...        1.0\n",
      "996  richard gere can be a commanding actor , but h...        1.0\n",
      "997  glory -- starring matthew broderick , denzel w...        1.0\n",
      "998  steven spielberg ' s second epic film on world...        1.0\n",
      "999  truman ( \" true - man \" ) burbank is the perfe...        1.0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                                                review  sentiment\n",
      "0    plot : two teen couples go to a church party ,...        0.0\n",
      "1    the happy bastard ' s quick movie review damn ...        0.0\n",
      "2    it is movies like these that make a jaded movi...        0.0\n",
      "3    \" quest for camelot \" is warner bros . ' first...        0.0\n",
      "4    synopsis : a mentally unstable man undergoing ...        0.0\n",
      "..                                                 ...        ...\n",
      "995  if anything , \" stigmata \" should be taken as ...        0.0\n",
      "996  john boorman ' s \" zardoz \" is a goofy cinemat...        0.0\n",
      "997  the kids in the hall are an acquired taste . i...        0.0\n",
      "998  there was a time when john carpenter was a gre...        0.0\n",
      "999  two party guys bob their heads to haddaway ' s...        0.0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                                                 review  sentiment\n",
      "0     films adapted from comic books have had plenty...        1.0\n",
      "1     every now and then a movie comes along from a ...        1.0\n",
      "2     you ' ve got mail works alot better than it de...        1.0\n",
      "3     \" jaws \" is a rare film that grabs your attent...        1.0\n",
      "4     moviemaking is a lot like being the general ma...        1.0\n",
      "...                                                 ...        ...\n",
      "1995  if anything , \" stigmata \" should be taken as ...        0.0\n",
      "1996  john boorman ' s \" zardoz \" is a goofy cinemat...        0.0\n",
      "1997  the kids in the hall are an acquired taste . i...        0.0\n",
      "1998  there was a time when john carpenter was a gre...        0.0\n",
      "1999  two party guys bob their heads to haddaway ' s...        0.0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# nltk.download('movie_reviews')\n",
    "\n",
    "\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "twitter_data_pos = pd.DataFrame(data, columns = ['review', 'sentiment'])\n",
    "\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    row = {'review': ' '.join(list(words)), 'sentiment': 1}\n",
    "    twitter_data_pos = twitter_data_pos.append(row, ignore_index=True)\n",
    "    \n",
    "twitter_data_neg = pd.DataFrame(data, columns = ['review', 'sentiment'])\n",
    "\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    row = {'review': ' '.join(list(words)), 'sentiment': 0}\n",
    "    twitter_data_neg = twitter_data_neg.append(row, ignore_index=True)\n",
    "\n",
    "twitter_data = pd.DataFrame(data, columns = ['review', 'sentiment'])\n",
    "\n",
    "twitter_data = pd.concat([twitter_data_pos, twitter_data_neg], ignore_index=True) \n",
    "print(twitter_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nltk twitter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_dataset(twitter_data)\n",
    "\n",
    "tokenize_padd()\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the real twitter api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ').lower()\n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes:\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def strip_all(text):\n",
    "    #text = text.replace('  ', ' ')\n",
    "    #text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def get_twitter_reviews(movie_name):\n",
    "    consumer_key=''\n",
    "    consumer_secret=''\n",
    "    access_token='1166798786-'\n",
    "    access_token_secret=''\n",
    "\n",
    "\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    search_words = \"#\" + movie_name + \" -filter:retweets\"\n",
    "    print(search_words)\n",
    "    date_since = \"2018-01-01\"\n",
    "\n",
    "    tweets = tw.Cursor(api.search,\n",
    "                  tweet_mode=\"extended\",  \n",
    "                  q=search_words,\n",
    "                  lang=\"en\",\n",
    "                  since=date_since).items(500)\n",
    "\n",
    "    count = 0\n",
    "    data = {'review': [], 'sentiment': []}\n",
    "    twitter_api_data = pd.DataFrame(data, columns = ['review'])\n",
    "\n",
    "\n",
    "    clear_tweets = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        row = {'review': strip_all(strip_all_entities(strip_links(tweet.full_text))), 'sentiment': 0}\n",
    "        twitter_api_data = twitter_api_data.append(row, ignore_index=True)\n",
    "    #     clear_tweets.append([strip_all(strip_all_entities(strip_links(tweet.full_text)))])\n",
    "\n",
    "\n",
    "    return twitter_api_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_movie(movie_name):\n",
    "    print(get_twitter_reviews(movie_name))\n",
    "    X_twitter_test, y_twitter_test = load_dataset(get_twitter_reviews(movie_name))\n",
    "    \n",
    "    rank = 0\n",
    "\n",
    "    token = Tokenizer(lower=False)\n",
    "    token.fit_on_texts(X_twitter_test)\n",
    "    X_twitter_test = token.texts_to_sequences(X_twitter_test)\n",
    "    X_twitter_test = pad_sequences(X_twitter_test, maxlen=max_length, padding='post')\n",
    "\n",
    "    y_pred = (model.predict(X_twitter_test, batch_size = 128) > 0.5).astype(\"int32\")\n",
    "\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "\n",
    "    for review in y_pred:\n",
    "        if review[0] == 0:\n",
    "            neg = neg + 1\n",
    "        if review[0] == 1:\n",
    "            pos = pos + 1\n",
    "\n",
    "    rank = 10 * pos / (pos+neg)\n",
    "    print('negative review', neg)\n",
    "    print('positive review', pos)\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create server to connect with FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [26/Apr/2021 20:53:39] \"\u001b[37mOPTIONS /movie HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "justice league\n",
      "#justiceleague -filter:retweets\n",
      "                                                review  sentiment\n",
      "0    its all connected how the arrowverse fulfilled...        0.0\n",
      "1    i finally was able to sit and take a photo of ...        0.0\n",
      "2    in there i was understood oswald hides his pai...        0.0\n",
      "3    dc comics has announced justice league infinit...        0.0\n",
      "4    Not gonna lie It wasn t good On the other hand...        0.0\n",
      "..                                                 ...        ...\n",
      "495                 we fight for https t co s5xiagq0m5        0.0\n",
      "496                     this song goes with everything        0.0\n",
      "497       wonder woman cosplayer https t co k28bu1dxkg        0.0\n",
      "498               here are amp spoiler free reviews of        0.0\n",
      "499  love lego games by while waiting for check out...        0.0\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "#justiceleague -filter:retweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Apr/2021 20:54:22] \"\u001b[37mPOST /movie HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative review 54\n",
      "positive review 446\n"
     ]
    }
   ],
   "source": [
    "import flask as flask\n",
    "from flask import Flask, jsonify, json, request\n",
    "from flask_restful import Api, Resource, reqparse\n",
    "from flask_cors import CORS\n",
    "import random\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "api = Api(app)\n",
    "\n",
    "class Api(Resource):\n",
    "    \n",
    "    @app.route(\"/movie\", methods=[\"POST\"])\n",
    "    def movie():\n",
    "        parser = reqparse.RequestParser()\n",
    "        parser.add_argument(\"movie\")\n",
    "        params = parser.parse_args()\n",
    "\n",
    "\n",
    "        print(params['movie'])\n",
    "        rank = rank_movie(params['movie'].replace(\" \", \"\"))\n",
    "        resp = flask.Response('{\"rank\":' + str(rank) + '}')\n",
    "#         resp = flask.Response(str(params['movie']))\n",
    "        resp.headers['Content-Type'] = 'text/plain'\n",
    "        resp.headers['Access-Control-Allow-Origin'] = 'POST'\n",
    "        return resp\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        from werkzeug.serving import run_simple\n",
    "        run_simple('localhost', 9000, app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
